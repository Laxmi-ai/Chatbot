# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ais9v6dE6Ykt1Zk3sPLhcQxVfa85XBl
"""

!pip install -q langchain faiss-cpu sentence-transformers transformers streamlit

from google.colab import files
uploaded = files.upload()

# Save the uploaded file path
file_path = list(uploaded.keys())[0]



from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Load document
loader = TextLoader(file_path)
documents = loader.load()

# Split into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = text_splitter.split_documents(documents)

# Embed using HuggingFace model
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(chunks, embedding_model)



!pip install pypdf

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Load document
loader = PyPDFLoader(file_path)
documents = loader.load()

# Split into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = text_splitter.split_documents(documents)

# Embed using HuggingFace model
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(chunks, embedding_model)

from langchain.llms import HuggingFacePipeline
from transformers import pipeline

# Load text-generation pipeline (lightweight model)
text_gen = pipeline("text-generation", model="tiiuae/falcon-rw-1b", max_new_tokens=200)
llm = HuggingFacePipeline(pipeline=text_gen)

from langchain.chains import RetrievalQA

retriever = vectorstore.as_retriever()
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

while True:
    query = input("Ask a question (type 'exit' to quit): ")
    if query.lower() == "exit":
        break
    answer = qa_chain.run(query)
    print("Answer:", answer)

